\chapter{Deck Recommendation in Collectible Card Games} 

\label{chapter:qdeckrec} 


\section{Introduction}\label{sec:introduction}

In this chapter, we investigate the development of a recommendation system for answering \hyperref[rq1]{\textbf{R.S.Q.~1}} in the \textbf{one-vs-one} setting, which is reiterated here:

\begin{equation}
  \tag{R.S.Q. 1}\label{chapter3_rq1}
  \parbox{\dimexpr\linewidth-4em}{%
    \strut
    How can we design systems working in the pre-match stage that can recommend winning-effective in-game elements, within the settings of one-vs-one and team-vs-team?
    \strut
  }
\end{equation}

As we stated in Chapter~\ref{chapter:intro}, our  motivation is to increase player competence through helping players make more winning-effective choices of in-game elements. The challenge here is that most modern video games offer a large number of in-game element choices, which makes it hard to select favorable ones. After surveying a few game genres, we find that recommendations of winning-effective starting items in one-vs-one \textit{Collectible Card Games} (CCGs) is a sufficiently complicated example for the aforementioned research question. While readers can refer to Section~\ref{sec:background_ccg} for a detailed background of CCGs, we give a brief description here for readers' convenience.

One-vs-one CCGs are match-based card games, where two players wield cards with various in-game effects to attack opponents and defend themselves. In a match, a player's cards are drawn from his or her own \textit{deck} - a deck is a collection of cards prepared before the match starts which represents specific play strategies a player may want to implement. CCGs are often designed with an abundant number of cards (a few hundreds to thousands), but the size limit of a deck is usually much smaller (a few dozens). Supposing the number of total cards and deck size are $N$ and $D$, respectively, there could be as many as ${N\choose D}=N!/D!/(N-D)!=O(N^D)$ ways to assemble decks. Besides, due to sophisticated synergistic and oppositional relationships among cards, there is no single deck which can universally win against all others. All these factors make the task of identifying winning effective decks very challenging for players. 


In this chapter, we proposed a recommendation system, named \textit{Q-DeckRec}, able to efficiently identify winning-effective decks for players in such a setting. The chapter is structured as follows. We first introduce the formal problem formulation and then analyze how existing methods are inefficient to solve the problem. Next, we illustrate our proposed algorithm. Finally, we show experiments and results to compare our method with others.

 
 \section{Problem Formulation}\label{sec:qdeckrec_probform}
We treated the problem of recommending a \textit{winning-effective} deck against an opponent as to approximately identify the \textit{winning-optimal} deck against that opponent. The winning-optimal deck is defined as the one with the highest expected win rate against the opponent. Each of the player (the recommendee) and the opponent is abstracted to a specific play style and deck. On a high level, the problem is to output the approximated winning-optimal deck for the player given both players' play styles and the opponent's deck.

As a deck is a combination of cards, identifying the winning-optimal deck can be formulated as a \textit{combinatorial optimization problem (COP)}. Let us briefly review the definition of COP. Generally, an optimization problem consists of an \textit{objective function} and a set of \textit{problem instances}. Each problem instance is defined by a set of variables and a set of constraints among those variables. A \textit{candidate solution} to a problem instance is an assignment of values to the variables. A \textit{feasible solution} is a candidate solution that satisfies the set of constraints. An \textit{optimal solution} is a feasible solution that maximizes the value of the objective function. A COP is an optimization problem whose problem instances have finite numbers of candidate solutions. For many realistic COPs such as the traveling salesperson problem (TSP), the number of candidate solutions is too large to exhaust in order to identify an optimal solution. This is exactly where our problem lies. In other words, the problem can be formulated as searching for the winning-optimal deck to achieve the highest expected win rate among a large, but finite number of deck candidates.

Following the previous notations, suppose a deck is a subset of cards of size $D$ among a total of $N$ cards, with $N > D$ because $N$ is usually several times larger than $D$. A deck can be represented as a binary vector of length $N$, $ \vect{x} \in \mathbb{Z}_2^N$, whose components of 1's correspond to the cards included in the deck and 0's otherwise. Since a deck has a fixed size of cards, we have $\|\vect{x}\|_1=D$. We use $\vect{x}_p$ and $\vect{x}_o$ to differentiate the deck of the player and his opponent. 

We use $\mathcal{A}_p$ and $\mathcal{A}_o$ to capture the play styles of the player and his or her opponent. $\mathcal{A}_p$ and $\mathcal{A}_o$ are play style-specific simulators that decide which cards to issue given a game instance, henceforth referred to as the \textit{AI proxies} (artificial intelligence) of respective play styles. The evaluation function $f(\cdot)$ is defined as $f(\vect{x}_p; \vect{x}_o, \mathcal{A}_p, \mathcal{A}_o)$, which returns the winning probability of the player using $\vect{x}_p$ against the opponent using $\vect{x}_o$, with their play styles following $\mathcal{A}_p$ and $\mathcal{A}_o$ respectively. Note that $f(\cdot)$ is a black-box function. We do not have the closed-form expression of $f(\cdot)$, but can approximate its value by simulating $\mathcal{A}_p$ and $\mathcal{A}_o$ playing against each other for several matches.

The objective function of finding the winning-optimal deck is formulated as:
\begin{align}
\begin{split}
\argmax_{\vect{x}_p} & \;\; f(\vect{x}_p; \vect{x}_o, \mathcal{A}_p, \mathcal{A}_o), \\
\text{subject to } \;\;&\vect{x}_p \in \mathbb{Z}_2^N, \vect{x}_o \in \mathbb{Z}_2^N, \\
&\|\vect{x}_p\|_1=\|\vect{x}_o\|_1=D
\end{split}
\label{eqn:obj}
\end{align}
the solution of which is denoted as $\vect{x}_p^*$.

In practice, the brute-force approach is infeasible to solve this problem, as we need to evaluate an $O(N^D)$ number of $\vect{x}_p$ configurations, while a sufficient number of matches need to be simulated to get a stable win rate estimation for each call of $f(\cdot)$. Since the simulation needs to apply numerous rules of the game on each move, this is a computationally demanding operation. In our experimental setting in Section~\ref{sec:qdeckrec_exp}, where $N=312, D=15$, we would need to exhaust around $1.4 \times 10^{25}$ possibilities if using brute-force. Each evaluation of $f(\cdot)$ was found to take non-negligible time in the order of seconds even on a very powerful server.   


% As a deck is a combination of cards, deck building can be formulated as a combinatorial optimization problem (COP), which relates to finding an optimal solution (the most winning-effective deck) in a finite search space of all possible decks. Deck building has a large and complex solution space. For example, the number of all possible decks in our experiment setting, which selects 15 out of 312 cards, is $1.4 \times 10^{25}$. 

\section{Analysis of Existing Methods}\label{sec:qdeckrec_existmethodanaly}
As we surveyed in Section~\ref{sec:rw_startitem}, previous works for approximately solving Eqn.~\ref{eqn:obj} are mainly search algorithms, which are divided into two categories: heuristic searches and metaheuristic searches. We will analyze below that the former might not be effective and the latter might not be efficient enough to be deployed as a winning-effective deck recommendation system.

Heuristic search methods decide which cards to include based on domain heuristics but requires intensive human knowledge and time to code heuristics considering various factors, such as the large number of cards, the requirement of finding a deck that is synergistic and oppositional, as well as accounting for different play styles. Furthermore, heuristics designed under limited manual resources may not be effective towards different opponents since they may only encode parts of information of card relationships and play styles - in CCGs subtle difference in just a few cards or play styles could lead to decks of very different strengths. Therefore, human generated heuristics are often hard to encode given the complexity of the game rules. Thus, there is a need for an automated and intelligent recommendation system that uses minimal labor, but can search such a large space efficiently. 

Metaheuristic searches refer to high-level, problem-independent, approximate search strategies for tackling optimization problems~\citep{birattari2009tuning}. In fact, it is not new to approximately solve COPs through metaheuristic searches. An example that we surveyed in Section~\ref{sec:rw_startitem} used a \textit{Genetic Algorithm} (GA)~\citep{holland1992adaptation} to evolve decks towards higher winning-effectiveness through repeated modifications and selections~\citep{garcia2016evolutionary,bjorke2017deckbuilding}. Although researchers have applied other kinds of metaheuristic searches on a variety of COPs, such as the \textit{Cross-Entropy} (CE) method~\citep{rubinstein1999cross}, tabu search~\citep{glover1986future}, and simulated annealing~\citep{kirkpatrick1983optimization}, GA has been the only one applied on the same COP as Eqn.~\ref{eqn:obj}, i.e., to approximately identify the winning-optimal deck. 

% The central idea is that the probability of locating an optimal solution using naive random search is a rare-event probability. CE can be used to obtain a new sampling distribution so that the rare-event is more likely to occur. Sampling from the new distribution will result in near-optimal solutions.

% However, they all require successive evaluations of intermediate solutions. because: (1) the search process requires a number of evaluations of candidate solutions; (2) the evaluation of a candidate solution's quality is computationally expensive, as this requires a large number of simulated matches with complicated in-game rules. 

% % Ideas proposed for deck building mainly fall into two categories: heuristics and metaheuristic searches. First, some heuristic methods decide which cards to include based on the popularity of cards from  historical data~\cite{frankkarsten,willfancher}. The underlying intuition is that popularly favored cards are very likely to be strong ones. Stiegler et al. propose a utility system to search deck with more types of game-specific heuristics besides card popularity, including mana curve, strategic parameters, cost effectiveness and card synergies~\cite{stiegler2016hearthstone}. However, all heuristics methods require intensive human knowledge, lack flexibility to adapt to a wide variety of opponents decks. As far as we know, \textit{Genetic Algorithm} (GA)~\cite{holland1992adaptation} has been the only metaheuristic search algorithm for deck building~\cite{garcia2016evolutionary,bjorke2017deckbuilding}. In their works, the fitness value is the average win rate of a candidate deck against a group of opponent decks while AI bots are used as a proxy for human play. However, we note in their results that a single run of GA for a particular deck building problem instance took hours or days to reach a winning-effective deck~\cite{garcia2016evolutionary,bjorke2017deckbuilding}. This is because each fitness evaluation requires obtaining a win rate based on a number of simulated matches. The complicated in-game rules also make the simulation computationally expensive. Therefore, non-learning search algorithms like GA are not practical for any large-scale or real-time deck recommendation task.

% In the previous works, the fitness value is the average win rate of a candidate deck against a group of opponent decks while AI bots are used as a proxy for human play. 

While metaheuristic search algorithms do not require human knowledge to guide searches, we find an inherent disadvantage to using them, which makes them inefficient for a deck recommendation system. They are \textit{non-learning} search algorithms, in the sense that each time a new problem instance arises they require many objective function evaluations until a sufficiently high-quality feasible solution is found. As we noted in Section~\ref{sec:qdeckrec_probform} and will show in the Experiments Section~\ref{sec:qdeckrec_exp}, the evaluation of the objective function (the simulation-based win rate estimation function $f(\cdot)$) is computationally expensive, because it requires a large number of simulated matches with complicated in-game rules. We note from the previous works of ~\textcite{garcia2016evolutionary} and \textcite{bjorke2017deckbuilding} that solving a single problem instance using GA often takes hours or days to reach a winning-effective deck. This means metaheuristic searches are not suitable for large-scale or real-time application of winning-effective deck recommendation, such as an online CCG's backend or a deck analysis website which serves a population of online players, because they would cause a heavy burden on computational resources and are intractable.

%  We expect several technical aspects to be thoroughly considered before such a method can be applied to our problem: (1) we need to prepare training datasets containing 

Additionally, researchers have also attempted to use supervised learning models to learn the mapping from problem instances (features) to optimal or approximately optimal solutions (labels)~\citep{vinyals2015pointer}. This approach has been applied in other COPs, but not to the problem of identifying winning-optimal decks. Optimal or approximated optimal solutions need to be calculated by some solver in advance in order to provide labels for supervised learning. One potential difficulty is to design special model architectures to cope with the discrete nature and constraints of COPs. For instance, in the TSP, the outputs should be constrained to sequences with no duplicated cities~\citep{vinyals2015pointer}. For our problem, it may be natural to think of using a multi-label classifier~\citep{tsoumakas2007multi}, in which the features are samples of opponent decks and the labels correspond to approximately winning-optimal decks obtained by existing methods, e.g., Genetic Algorithm. Still, there are several technical issues worth considering: (1) how to constrain the size of labels to match the size of a deck; (2) how to sample training data; (3) whether to further model dependencies among labels~\citep{zhang2010multi}, because of existence of complicated card relationships. Thinking thoroughly about these technical issues is non-trivial, hence we do not consider using supervised learning models for our problem in this chapter. Future work can explore the difference between our approach presented here and the use of supervised learning. 

% Metaheuristic search algorithms can also be viewed as conducting a state search. Unfortunately, they do not generalize a search policy. Instead, they rely on evaluating the qualities of the current state or next states before making operator choices based on metaheuristic rules independent of states.

% In Section~\ref{sec:method}, we will show that under certain assumptions, a deck building problem can be formulated as a COP and a search policy learned by RL  on "training" problem instances will quickly guide building winning-effective decks on future problem instances. 

% Besides metaheuristic and RL algorithms, problem-dependent heuristics can be used to search solutions when COPs have exploitable characteristics. 


\section{Methodology}\label{sec:method}
In this section we present our proposed solution, \textit{Q-DeckRec}, which is expected to solve Eqn.~\ref{eqn:obj} more efficiently than the heuristic search and metaheuristic search methods that we analyze in Section~\ref{sec:qdeckrec_existmethodanaly}. 


% In this paper, we focus on building winning-effective decks against specific individual opponents. We will leave the discussion about building decks against a group of opponents in Section~\ref{sec:conclusion}. 

% In this section, we introduce \textit{Q-DeckRec} for optimizing problem instances of Eqn.~\ref{eqn:obj} given that $\mathcal{A}_p$ and $\mathcal{A}_o$ are pre-trained and fixed.

% Under our assumption that $\mathcal{A}_p$ and $\mathcal{A}_o$ are pre-trained and fixed, card synergy and opposition relationships can be generalized. For example,  $\mathcal{A}_p$ is good at using Card A to counter $\mathcal{A}_o$, then Card A can be a .

% the opposition relationship between Card A and B will remain as a similar factor for all problem instances with the same AI proxies $\mathcal{A}_p$ and $\mathcal{A}_o$. What motivates us is that a deck building solver could generalize these relationships from solving previous problem instances. For future problem instances, it can exploit the generalized relationships to build decks.

% As a COP, the deck building problem can be solved by state search in a state space. 

% although different problem instances may use different $\mathcal{A}_p$ and $\mathcal{A}_o$, we assume that $\mathcal{A}_p$ and $\mathcal{A}_o$ come from a pool of AI proxies pre-trained by a deck recommendation system. 

Our method is inspired by a fundamental assumption that will be made and exploited: in practice, players' play styles come from a pool of AI proxies pre-trained by a deck recommendation system. This is a reasonable assumption because a deck recommendation system may not have sufficient data and resource to approximate every individual player's play style; rather, it is more tractable to cluster a player's play behavior into one of several pre-trained AI proxies. For example, each AI proxy from the pool represents a specific play style archetype such as "aggressive" or "conservative". Under this assumption, each problem instance consists of $\vect{x}_o$, which may vary, and $\mathcal{A}_p$ and $\mathcal{A}_o$, which are available. Therefore, there may exist deck building patterns which can be generalized for each pair of AI proxies in an offline stage. For example, if certain $\mathcal{A}_p$ is good at using Card A to counter certain $\mathcal{A}_o$, then Card A tends to appear in the optimal solution of many problem instances with the two AI proxies as the input. In the rest of the chapter, we assume that we deal with problem instances of Eqn.~\ref{eqn:obj} under a specific pair of $\mathcal{A}_p$ and $\mathcal{A}_o$. Our method
will be invariantly applied for other pairs of AI proxies.

In the previous works based on GA~\citep{garcia2016evolutionary,bjorke2017deckbuilding}, the search process is independent among different problem instances, with no information generalized for future problem instances to reduce and avoid the evaluation of the objective function. Since now we assume there could be generalizable deck building patterns under a pair of $\mathcal{A}_p$ and $\mathcal{A}_o$, it is natural for us to look for models that can encode those patterns and facilitate the solving process of future problem instances.

Our inspiration above aligns with the algorithms which learn search policies for solving optimization problem instances~\citep{zoph2016neural,li2017learning,chenlearning,zhang2000solving,bello2016neural}. These algorithms lie in a broader area known as "meta-learning"~\citep{lemke2015metalearning,brazdil2008metalearning,vilalta2002perspective} or "learning to learn"~\citep{thrun2012learning}. Learning of search policies relies on viewing the optimization process as conducting sequential decision making~\citep{littman1996algorithms} by an optimizer agent. The optimizer agent starts in some initial state and consecutively applies operators to move to new states. The goal is to end at a final state where a high-quality feasible solution can be extracted. We use the mapping between states and operator choices as the \textit{search policy}. 

If we additionally define a transition function and a reward function, we can formulate the optimization process as a \textit{Markov Decision Process} (MDP)~\citep{bellman1957markovian}. The optimal policy that maximizes long-term rewards can be learned or approximated by leveraging reinforcement learning (RL) algorithms~\citep{sutton1998reinforcement}. The key is to properly design the MDP, especially the reward function, such that the learned policy can guide the optimizer agent quickly towards high-quality feasible solutions. For example, \textcite{zhang2000solving} applied an RL algorithm $TD(\lambda)$ to obtain the search policy for solving NASA space shuttle scheduling problem instances. Their results show that the learned search policy is more effective in the ratio of solution quality vs. CPU time than the best known non-learning search algorithm on test problem instances. \textcite{bello2016neural} show that a search policy parameterized as a special structure of neural network can be trained and used to solve unseen instances of TSP. 

Inspired by meta-learning algorithms, we proposed to delegate solving the COP of Eqn.~\ref{eqn:obj} as a problem of generalizing a search policy in an MDP environment, where an agent navigates in the state space to search for the winning-optimal deck and its search policy generalizes deck building patterns under a pair of AI proxies $\mathcal{A}_p$ and $\mathcal{A}_o$. 

In the MDP, a state $s \in \mathcal{S}$ consists of a unique feasible solution $\vect{x}_p$, together with $\vect{x}_o$ and a step counter $t$ as complement information, i.e., $s=\{\vect{x}_p, \vect{x}_o, t\}$. An action $a \in \mathcal{A}$ is defined as a card replacement to modify the current deck $\vect{x}_p$. An action replaces exactly one card in the deck $\vect{x}_p$ with another card not included in $\vect{x}_p$. One special action is to keep the current deck as unmodified. Given the actions we define, the transitions between states $T: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S}$ are always deterministic. One state applied by an action will transit to only one next state, reflecting the corresponding card modification, denoted as $\{\vect{x}^{(t)}_p, \vect{x}_o, t\}, a \rightarrow \{\vect{x}^{(t+1)}_p, \vect{x}_o, t+1\}$. The deck search starts from a random initial state $s_0=\{\vect{x}_p^{(0)}, \vect{x}_o, 0\}$ and is limited to take exact $D$ actions in one episode. We denote the states within one episode as $s_0, s_1, \cdots, s_D$. We limit the length of the horizon to be $D$ because at most we need to replace all the cards in $\vect{x}_p^{(0)}$ to reach the optimal deck $\vect{x}_p^*$. 


% The mapping between states and operator choices is called the \textit{search policy}. Non-learning search algorithms rely on calling $f(\cdot)$ to evaluate the quality of states in order to make operator choices. As we stated, each call of $f(\cdot)$ takes non-negligible computational resources. During search, non-learning search algorithms do not memorize or generalize a search policy. This means next time the agent is at the same state or a similar state, it still needs to call $f(\cdot)$ for state evaluation. Searching a winning-effective deck can be viewed as an agent applying operators and navigating in the state space. The goal of the agent is to end at a high-quality state. Since we assume there are generalizable deck building patterns, a more efficient approach for the agent is to generalize a search policy from the search in previous problem instances and follow it without calling $f(\cdot)$ for future problem instances.  

% Continuing on the previous example, suppose there are two deck building problem instances. The two instances have very similar $\vect{x}_o$, both with Card C. Suppose during the search for the first instance, the agent discovers the decks with Card A and B are winning-effective. Then a. 

% The problem remains as how to learn a search policy. We propose to learn a search policy in the state search as a reinforcement learning (RL) problem. RL algorithms aims to identify or approximate an optimal policy on an environment characterized as a Markov Decision Process (MDP)~\cite{bellman1957markovian}. An MDP consists of a state space $\mathcal{S}$, an action space $\mathcal{A}$, a transition function $T: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S}$, and a reward function $R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$. The optimal policy is a policy which maximizes a defined long-term reward criterion. The  key is to properly design the MDP, especially the reward function and long-term reward criterion, such that the optimal policy is indeed the desired search policy which can lead to winning-effective decks from any state.


% We propose to learn a search policy in the state search as a reinforcement learning (RL) problem. RL algorithms aims to identify or approximate an optimal policy on an environment characterized as a Markov Decision Process (MDP)~\cite{bellman1957markovian}. An MDP consists of a state space $\mathcal{S}$, an action space $\mathcal{A}$, a transition function $T: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S}$, and a reward function $R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$. 

% Formally, solving the deck building COP by sequential decision making is formed as an MDP as follows. A state $s \in \mathcal{S}$ consists of $\vect{x}_p$, $\vect{x}_o$ and a step counter $t$, i.e., $s=\{\vect{x}_p, \vect{x}_o, t\}$. An action $a \in \mathcal{A}$ is defined as a card replacement to modify the current deck $\vect{x}_p$. An action replaces exactly one card in the deck $\vect{x}_p$ with another card not included currently. One special action is to keep the current deck as unmodified. Given the actions we define, the transitions between states are always deterministic. One state applied by an action will transit to only one next state, reflecting the corresponding card modification, denoted as $\{\vect{x}^{(t)}_p, \vect{x}_o, t\}, a \rightarrow \{\vect{x}^{(t+1)}_p, \vect{x}_o, t+1\}$.


The problem remains as: how to design the reward function $R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$. In the MDP, the optimal policy is the one which maximizes a defined long-term reward criterion. The  key is to properly design the reward function and long-term reward criterion, such that the optimal policy is indeed the desired search policy which can lead to winning-effective decks from any state. 

The long-term reward criterion defines the goal of reinforcement learning. It should encourage the optimal policy to search in the direction of winning-effective decks. We proposed the following long-term reward criterion for each episode:
\begin{align}
R &=\sum_{t=0}^{D-1} r_t,
\label{eqn:cumulative_sum_reward}
\end{align}
where $r_t$ is the reward function over each transition. Specifically, we defined $r_t$ as the win rate between the opponent deck and the modified deck after step $t$ with exponential amplification:
\begin{align}
    r_t=exp(b \cdot f(\vect{x}^{(t+1)}_p; \vect{x}_o, \mathcal{A}_p, \mathcal{A}_o)),
\end{align}
where $b$ is a positive constant to adjust the extent of amplification. We chose this reward function over $r_t=f(\vect{x}^{(t+1)}_p; \vect{x}_o, \mathcal{A}_p, \mathcal{A}_o)$ in order to amplify the difference between strong and weak decks. Although the goal of deck building is to land on ${s_D=\{\vect{x}_p^{(D)}, \vect{x}_o, D\}}$ with $r_D=f(\vect{x}_p^{(D)}; \vect{x}_o, \mathcal{A}_p, \mathcal{A}_o)$ as high as possible, the cumulative sum of win rates as in Eqn.~\ref{eqn:cumulative_sum_reward} provides more reward signals along the entire search trajectory than merely optimizing ${R=r_D}$. For similar reasons, the form of cumulative sum in reward functions has also been adopted in other optimization problems based on sequential decision making~\citep{andrychowicz2016learning,chenlearning}. Since we modeled each episode with finite horizons, we ignored the conventional reward discount factor $\gamma$ in the definition of $R$, which is a mathematical trick to help the convergence of RL learning in MDPs with infinite horizons. 

The optimal policy can be obtained by always selecting the action with the highest optimal state-action value at each state:
\begin{align}
    \pi^*(s) = \argmax_a Q^*(s,a),  s = s_0, \cdots, s_{D-1},
\label{eqn:optpolicy}
\end{align}
where $Q^*(s,a)$ is defined as the best state-action value function among all possible policies:
\begin{align}
Q^\pi(s,a)=\mathbb{E}[\sum_{i=t}^{D} r_i | s_t=s, a_t=a, \pi] 
\end{align}
\begin{align}
Q^*(s,a)=\max_\pi Q^\pi(s,a) 
\end{align}
The larger $Q^*(s,a)$ is, the larger the upper bound of the cumulative rewards the agent would collect until the end of the episode. Intuitively, $Q^*(s,a)$ is a proxy of how winning-effective the final deck $\vect{x}_p^{(D)}$ could maximally be after applying the modification $a$ on the current deck encoded in $s$. Therefore, following $\pi^*(s)$ generates a series of modifications that faithfully build the winning-optimal deck. 

We proposed to use a Reinforcement Learning (RL) algorithm, Q-Learning~\citep{watkins1992q}, to learn $Q^*(s,a)$ iteratively through observation tuples $(s,a,r,s')$. The simplest implementation of Q-Learning is a look-up table and a learning rate $0 < \alpha \leq 1$, with the update rule as: 
\begin{align}
\hat{Q}(s,a) = (1 - \alpha) \hat{Q}(s,a) + \alpha (r + \max_{a'} \hat{Q}(s', a')) 
\label{eqn:qsa}
\end{align}

Theory implies that if each action is tried in each state an infinite number of times and the magnitude of $\alpha$ meets certain criteria, then $\hat{Q}$ converges to $Q^*$~\cite{bertsekas1989parallel}. However, our problem has a huge state space, hence it is not possible to maintain a look-up table for all combinations of states and actions. Instead, we resort to Multi-Layer Perceptron (MLP) with parameters $\theta$ as a function approximator: $Q_\theta(s,a)$ learns to approximate the mapping of the feature representation of the state-action pair, $\mathcal{F}(s,a)$, to the optimal state-action value, $Q^*(s,a)$. More specifically, we use an MLP architecture with one input layer, one hidden layer and one output layer. Without requiring any prior domain knowledge, we simply let $\mathcal{F}(s,a)=s'$. Therefore, the input layer takes as input a state representation $s'$, which has ${2\cdot N+1}$ dimensions. The output layer outputs a real value  representing the predicted $Q^*(s,a)$. The exact specifications can be seen in Section~\ref{sec:qdeckrec_exp}. The update rule of $\theta$ is in a gradient descent fashion towards reducing so-called TD-error $\delta$:

\begin{align}
\delta := r + \max_{a'} Q_{\theta}(s', a') - Q_{\theta}(s,a)
\label{eqn:tderror}
\end{align}

\begin{align}
\theta \leftarrow \theta + \alpha \cdot \delta \cdot \nabla Q_\theta(s,a)
\label{eqn:updaterule}
\end{align}

To learn $\theta$, we need to collect  observation tuples $(s,a,r,s')$ through solving "training" problem instances. Solving a training problem instance is to let Q-DeckRec take actions $D$ times based on the current $Q_\theta(s,a)$ function in an episode. In order to generalize $Q_\theta(s,a)$ to various states, we initialize both $\vect{x}_o$ and $\vect{x}_p^{(0)}$ in $s_0$ randomly at the beginning of each episode. An $\epsilon$-greedy policy is used during the training, with $\epsilon$ slowly decreasing as the learning proceeds. The policy has $\epsilon$ probability to choose non-optimal actions in the hope to escape any local optimum and discover better policies. 

Also, we used prioritized experience replay~\citep{schaul2015prioritized} to improve sample efficiency. Past experiences (i.e., observation tuples) are stored and weighted according to the absolute value of $\delta$. High TD-error associated experiences will be more likely to be sampled for MLP parameter learning. High TD-error associated experiences are those ``surprising'' or unexpected observations that the current values of the MLP parameters could explain less well. Sampling them frequently helps the learning process focus on the hardest samples. Prioritized experience replay has been shown to speed up parameter learning in several classic RL benchmarks compared to uniform experience sampling methods~\citep{schaul2015prioritized}.  

The training phase of Q-DeckRec can be summarized as follows. At the beginning of each training episode, both $\vect{x}_o$ and $\vect{x}_p^{(0)}$ are randomly generated. Q-DeckRec decides how to "navigate" through states by $\epsilon$-greedy policy and $Q_\theta(s,a)$ in $D$ steps. All the $D$ transitions are stored into the prioritized experience replay pool. Following that, $m$ previous observation tuples $(s, a, r, s')$ are sampled from the prioritized experience replay as a learning batch for updating $\theta$ as described in Eqn.~\ref{eqn:tderror}~and~\ref{eqn:updaterule}. The loop continues after a new training episode is initiated. The training is terminated after a time limit is reached. 

After training, $Q_\theta(s,a)$ is fixed. When solving a future problem instance, Q-DeckRec can start from $s_0$ with a random $\vect{x}_p^{(0)}$ and follows $\pi^*$ as in Eqn.~\ref{eqn:optpolicy} in $D$ steps. No call of $f(\cdot)$ is needed during the search. As a comparison, non-learning search algorithms such as Genetic Algorithm require calling $f(
\cdot)$ multiple times in order to evaluate fitness values for each problem instance~\citep{garcia2016evolutionary,bjorke2017deckbuilding}, while calling $f(\cdot)$ would take computational resources much heavier than calculating $Q_\theta(s,a)$. Therefore, Q-DeckRec has its superior suitability for large-scale or real-time application.

\section{Experiment Setup}\label{sec:qdeckrec_exp}
To test our method and compare it with other methods, we experimented on an open-sourced CCG simulator \textit{MetaStone}\footnote{\url{https://github.com/demilich1/metastone}}, which is  based on the popular online digital CCG \textit{Hearthstone} (Blizzard Entertainment, Inc.). All experiments run on a powerful server with Intel E5 2680 CPUâ€™s @ 2.40 GHz (56 logical CPU cores). Parallelization is implemented in three places: (1) linear algebra operations used in the MLP in Q-DeckRec; (2) match simulations evenly spread on all cores when evaluating $f(\cdot)$; (3) random deck sampling from a baseline based on Monte Carlo simulations (introduced later). Each call of $f(\cdot)$ returns a win rate based on 300 simulated matches, which on average takes 5 seconds and has around 5\% standard deviation in the win rate evaluation. 

% We expect the experimental results to generalize under other settings.

We made a few decisions in setting up our experiments. First, we used the same AI proxy to represent both $\mathcal{A}_p$ and $\mathcal{A}_o$. The AI proxy, provided by the simulator and called \textit{GreedyOptimizeMove}, is a model-based method which decides the best action by evaluating each action's consequence according to a heuristic. Choosing the same AI proxy for both players is just for simplicity purposes and we expect that the superior efficiency of Q-DeckRec to other methods, which is to be shown in our experiments, can generalize to other pairs of AI proxies. As a side note, we did not choose to use other AI proxies based on  search-based methods~\citep{santos2017monte} because they take much longer to complete the simulation of one match. However, there have been attempts to train model-based AI proxies which can play comparably with search-based methods but with faster speed~\citep{janusz2017helping}. Second, we assume both players are from a specific in-game character class called \textit{Warriors}. The total number of available cards to Warriors is 312. Third, while in the real game certain cards can have at most one copy and all other cards can have at most two copies in the deck, we impose that every included card has two copies. This reduces our search space size for testing purpose, and also follows the postulation that having two copies for every card makes the deck performance more reliable~\citep{garcia2016evolutionary,garcia2018automated}. As a result, although the deck size is 30, the number of cards to be selected is 15. In summary, we have $N=312, D=15$ when optimizing Eqn.~\ref{eqn:obj}. Different CCGs have different deck sizes and numbers of total cards and there do exist CCGs with larger search space than this setting. We plan to test more combinations of $N$ and $D$ in the future.
    
We set up Q-DeckRec as follows. The underlying MLP has one hidden layer and one output layer. The hidden layer consists of 1000 rectified linear units (ReLU). The output layer is a single unit which outputs a weighted sum from the activation values of the hidden layer. $\epsilon$ in the $\epsilon$-greedy policy starts at $1$ and decreases $0.0005$ per training episode until it reaches $0.2$. The size of a learning batch, $m$, is set at 64. For the prioritized experience replay~\citep{schaul2015prioritized}, the exponent $\alpha$ is set at 0.6, the exponent $\beta$ is linearly annealed from $\beta_0=0$ to $1$ with step $1e^{-5}$. The capacity of the experience pool is 100K. The constant $b$ in the reward function is set as 10. All the hyperparameters are chosen empirically without fine-tuning due to large computational resources required.  

We compared Q-DeckRec with a Genetic Algorithm (GA), the method used in previous works for solving similar problems~\citep{garcia2016evolutionary,bjorke2017deckbuilding}. We implement GA with an open source library \textit{DEAP}\footnote{\url{https://github.com/DEAP/deap}}. An individual is a candidate deck $\vect{x}_p$. The fitness value is $f(\vect{x}_p; \vect{x}_o, \mathcal{A}_p, \mathcal{A}_o)$. The mutation and crossover functions are customized to maintain the validity of individuals, similarly to what was adopted in~\textcite{bjorke2017deckbuilding}. Specifically, mutation is swapping one card in the deck with one not in the deck and crossover randomly exchanges cards not overlapped by the two decks. The population size of each generation is 10, with the mutation probability and the crossover probability both set as 0.2. Individual selection is based on a commonly used selection mechanism called \textit{tournament} of size 3. 

% Following the idea of learning the mapping between problem instances and optimal solutions by supervised learning models~\cite{vinyals2015pointer}, as we note in the end of Section~\ref{sec:qdeckrec_existmethodanaly}, we implement a simple multi-label classifier (ML) based on an MLP architecture. The training data involve random samples of opponent decks $\vect{x}_o$ and corresponding approximated winning-optimal decks $\vect{\hat{x}}^*_p$. Each $\vect{\hat{x}}^*_p$ is obtained by applying a GA specified in the last paragraph for consecutive generations until the fitness value does not improve for over 10 generations. The input layer of the MLP has the same size as $N$, which consumes samples of $\vect{x}_o$. The hidden layer is the same as that in Q-DeckRec, i.e., 1000 ReLU units. The output layer is $N$ sigmoid units which are fitted against $\vect{\hat{x}}^*_p$. When solving a new problem instance  after training, the model takes the $\vect{x}_o$ as input, and builds $\vect{x}_p$ with the cards corresponding to the $D$ highest output values. 

We also designed an ad-hoc baseline which, like Q-DeckRec, requires a learning phase and does not require calling $f(\cdot)$ for solving future problem instances. The baseline conducted Monte Carlo (MC) simulations using a win rate predictor $\hat{f}(\cdot)$ to locate a solution. We first trained a supervised learning model to approximate $f(\cdot)$. The training data are randomly generated pairs of decks represented as binary vectors. The labels are the evaluated win rates based on $f(\cdot)$. We chose to train an MLP with the same architecture as in Q-DeckRec. Given the same input, $\hat{f}(\cdot)$ would output faster than $f(\cdot)$ because the former does not need a real match simulation. When solving a future problem instance with opponent deck $\vect{x}_o$, we ran MC simulations according to:

\begin{align}
\argmax_{\vect{x}_p \in \mathcal{X}_p} \hat{f}(\vect{x}_p, \vect{x}_o; \mathcal{A}_p, \mathcal{A}_o),
\label{eqn:mcmaxobj}
\end{align}
where $\mathcal{X}_p$ is a set of randomly generated decks. We denote the size of $\mathcal{X}_p$ as $X$. A larger $X$ means more thorough sampling.

In these experiments, there are several methods that we did not include or compare with, but instead we left that to future work. Specifically, we did not include any heuristic search method, because we focused on algorithmic deck recommendation systems requiring minimal human knowledge involved. Besides GA, we did not include other metaheuristic search methods; similar to GA, they all require calling the win rate evaluation function $f(\cdot)$ several times while solving each problem instance. We did not include supervised learning models, which directly learns the mapping from problem instances to optimal solutions, because to do so requires designing a specific model architecture to cope with the characteristics of the deck recommendation problem (e.g., outputs are constrained to contain $K$ cards), which has not been studied before and requires non-trivial extra works.

% In light of this, in the rest of the paper, an algorithm refers to a specific approach (GA or Q-DeckRec) under a specific wall time limit. 
Different wall time (i.e., real elapsed time) limits are imposed as the termination condition for both Q-DeckRec training and GA solving one problem instance. In this way, we can compare how long Q-DeckRec training and a GA run would take to reach similar performances. Wall time limits were chosen empirically based on observations in preliminary experiments and our limited computational resources. For GA, we tried wall time limits as 10, 15, 20 and 25 minutes because performances often plateau after 20 minutes, as evidenced in the result section (Section~\ref{sec:result}). Note that the time limit for GA is in the order of minutes because GA takes hundreds of objective function evaluations during evolution for each problem instance before the fitness function plateaus (to be shown in Table~\ref{tab:ga}) and each call of $f(\cdot)$ takes around 5 seconds as we tested. Since we did not have the optimal solutions for test problem instances, we reference the solutions from 25-minute GA searches as approximated ground truths. For Q-DeckRec training, we tested one, two and three days as the wall time limit. As will also be shown in Section~\ref{sec:result}, Q-DeckRec after a three-day training period can already reach the same optimal solution reached by GA with 25 minute limit for solving one problem instance.

For the MC-simulation method, we used a training data set collected in three days and test ${X=67, 670, 6.7K, 67K, 670K}$ and $6700K$. Note that $67K$ is around the same number Q-DeckRec calls its learned function approximator $Q_\theta(s,a)$ for solving a test problem instance\footnote{As in Eqn.~\ref{eqn:optpolicy}, each optimal action is decided after calculating the state-action values of all possible actions ($(N-D)\cdot D + 1$) and we need to take $D$ actions per episode. When $N=312$ and $D=15$, the total number of state-action value evaluations is 66840.} whereas higher values of $X$ than $6700K$ would require too large computational resources to be practical for large-scale or real-time application. 

% For the multi-label (ML) classification method, we use three training datasets collected in one, two, and three days, which amounts to $$, $$, $512$ pairs of $(\vect{x}_o, \vect{\hat{x}}^*_p)$, respectively.

In the rest of this chapter, we will label the algorithms we discussed based on the specific approach, such as $GA$, $Q\textnormal{-}DeckRec$, or $MC$, plus an associated parameter. For example, $GA_{20min}$ denotes Genetic Algorithm with 20-minute limit for solving one problem instance; $MC_{670K}$ denotes the Monte Carlo simulation-based method with $670K$ randomly generated decks; $Q\textnormal{-}DeckRec_{1day}$ and $Q\textnormal{-}DeckRec_{2days}$ are two different algorithms denoting $Q\textnormal{-}DeckRec$ with training time as 2 days and 3 days, respectively.   

To evaluate and compare all algorithms, we generated 20 test problem instances as follows. In our preliminary experiments where test problem instances were randomly generated, we found GA often only needed less than 100 calls of $f(\cdot)$ to identify decks with 100\% win rate. This is because randomly generated $\vect{x}_o$ barely had any effective card synergy and could be easily beaten by a mediocre deck. In real-world applications, we believe it is more demanding to build winning-effective decks against competitive decks rather than random decks. In order to generate competitive opponent decks as test problem instances, we adopted a sequential manner: we sampled a deck $\vect{x}$ from the outputs of all algorithms for the last test problem instance, where the sampling distribution is weighted by $f(\vect{x};\vect{x}_o, \mathcal{A}_p, \mathcal{A}_o)$. We then used $\vect{x}$ as the input $\vect{x}_o$ for the next problem instance. The first test problem instance was obtained after 10 preliminary runs. 

The performance of an algorithm is measured as follows. We ran each algorithm on each test problem instance 10 times. Each run was associated with a random seed, which controlled the initialization of $\vect{x}_p^{(0)}$ in $s_0$ in Q-DeckRec, and the randomness in evolution behaviors in GA. Then, we used the median of the 10 runs as the performance for the algorithm on the problem instance. To measure the significance of the differences for each pair of algorithms, we also conducted a two-tailed paired Welch's t-tests with a confidence level 0.01 over all test instances. The null hypothesis is that the mean difference between the paired algorithms' win rates is zero.

In order to give a complete view of resource usage, we recorded both wall time and CPU time each algorithm takes to solve a test problem instance.

\begin{table}
\caption{Results of Genetic Algorithm Approach} % title name of the table
\centering % centering table
\begin{tabular}{c c c} % creating 10 columns
\hline
 \begin{tabular}{@{}c@{}} Search Time \\ Wall Time / CPU Time\end{tabular} &  Func. Calls &  Win Rate
\\ 
\hline 
% Entering 1st row
10 min / 6.2 hr & 110  & 0.61 \\
15 min / 9.3 hr & 189 & 0.86 \\
20 min / 12.8 hr & 267 & 0.93 \\
25 min / 15.9 hr & 315 & 0.94 \\
\hline 
\end{tabular}
\label{tab:ga}
\end{table}


\begin{table}
\caption{Results of Q-DeckRec} % title name of the table
\centering % centering table
\begin{tabular}{c c c c} % creating 10 columns
\hline
 \begin{tabular}{@{}c@{}} Training \\ Wall Time\end{tabular} & \begin{tabular}{@{}c@{}} Search Time \\ Wall / CPU Time\end{tabular} & Func. Calls &  Win Rate \\
\hline
1 day & & 20K & 0.64 \\
2 days & 0.38 sec / 9.63 sec & 41K & 0.88 \\ 
3 days &  & 62K & 0.93  \\ 
\hline

\hline % inserts single-line
\end{tabular}
\label{tab:qdeckrec}
\end{table}

\begin{table}
\caption{Performances of MC simulation approach} % title name of the table
\centering % centering table
\begin{tabular}{c c c} % creating 10 
\hline
$X$ (Number of Samples) &  \begin{tabular}{@{}c@{}} Search Time \\ Wall Time / CPU Time\end{tabular} &  Win Rate
\\
\hline 
$67$ & 0.01 sec / 0.18 sec & 0.48  \\
$670$ & 0.03 sec / 0.91 sec & 0.64 \\
$6.7K$ & 0.05 sec / 2.16 sec & 0.75 \\
$67K$ & 0.45 sec / 8.45 sec & 0.84 \\
$670K$ & 4.90 sec / 97.60 sec & 0.82 \\
$6700K$ & 36.06 sec / 1031.81 sec & 0.77 \\
\hline 
\end{tabular}
\label{tab:mc}
\end{table}


\section{Results and Discussion}\label{sec:result}

The performances of the three kinds of methods ($GA$, $Q\textnormal{-}DeckRec$, and $MC$) are reported in Table~\ref{tab:ga},~\ref{tab:qdeckrec} and~\ref{tab:mc}. All the reported numbers are the means of performance over the 20 test problem instances. As stated, the performance for each test problem instance is the median of 10 runs. Also, we find that all pairwise comparisons on the win rate are significant, \textit{except}: (1) $GA_{20min}$ vs. $GA_{25min}$ (2) $Q\textnormal{-}DeckRec_{3days}$ vs. $GA_{20min}$ (3) $Q\textnormal{-}DeckRec_{3days}$ vs. $GA_{25min}$.

First, we observe that the performances of GA and Q-DeckRec improve as the wall time limits increase in our test ranges. This meets our expectation, because approximate COP solvers are supposed to get better solutions when using more computational resources. However, longer wall time limits than 20 minutes bring diminishing improvement in GA, as we find there is no significant difference in the average win rate between $GA_{20min}$ and $GA_{25min}$. 

From Table~\ref{tab:ga}, we observe that GA calls the win rate evaluation function an increasing number of times as the wall time limit increases. As we stated, the win rate evaluation is computationally expensive involving simulating 300 matches. Therefore, all GA algorithms require high CPU time in the order of hours. 

% only needs one ten-thousandth of what GA with 25 minute long search requires ($\approx$ 9.63 sec/15.9 hr).

As shown in Table~\ref{tab:qdeckrec}, Q-DeckRec can solve deck building problem instances with as little computational cost as 9.63 seconds in CPU time. Meanwhile, Q-DeckRec after 3-day training can build decks as winning-effective as $GA_{25min}$ does, as evidenced by the non-significant difference between $Q\textnormal{-}DeckRec_{ 3days}$ vs. $GA_{25min}$. Therefore, from the CPU time perspective, Q-DeckRec is much more efficient than GA ({9.63 sec $\ll$ 15.9 hr}), when solving a new problem instance, because the computationally heavy match simulations have been "moved" to the training phase. This proves the merit of Q-DeckRec being a suitable deck recommendation system for large-scale or real-time application. 

The number of function calls was $62K$ during the training of $Q\textnormal{-}DeckRec_{3days}$. This means there were $62K$ state transitions generated from roughly $4K$ ($\approx 62K/15$) training episodes. Even if each of the $62K$ state transitions was unique, they still involved a tiny fraction of total states in our formulated state space. (The number of total states is the number of possible opponent decks times the number of possible player decks: ${N\choose D} \times {N\choose D} \approx 1.97 * 10^{50}$.) This shows that the MLP-based architecture is a well-chosen function approximator for generalizing state-action values.    

% , when the MC simulation is based on 67K randomly generated decks

For the MC-simulation method, we first report Mean Squared Error (MSE) and $R^2$ of the learned supervised learning model. We evaluated them using a standard 10-fold cross validation. On training data, $MSE=0.005$ and $R^2=0.86$. On testing data, $MSE=0.008$ and $R^2=0.79$. To our surprise, from Table~\ref{tab:mc}, we find that the win rate does not monotonically increase as $X$ increases. The performance peaks at 0.84, which is significantly lower than $Q\textnormal{-}DeckRec_{3days}$. While debugging the method, we observe that the predicted win rate (the outcome of Eqn.~\ref{eqn:mcmaxobj}) monotonously increases as $X$ increases. We suspect that since the supervised learning model cannot perfectly predict the real win rate, deck samples inevitably contain outlier decks with spuriously high predicted win rates. These outlier decks "tricked" the MC-simulation method to select them unfortunately. The results showed that the approach of building winning-effective decks in a sequential way as in Q-DeckRec is more robust.


\section{Summary}\label{sec:conclusion}
In this chapter, we proposed a deck recommendation system named \textit{Q-DeckRec} for one-vs-one Collectible Card Games, which is able to approximately identify winning-optimal decks in large-scale and real-time after a period of training and requires minimal domain knowledge. We designed experiments which demonstrate the advantages of this approach. Hence, the proposed work answers \hyperref[rq1]{\textbf{R.S.Q.~1}} by showing an effective and efficient starting item recommendation system in the setting of one-vs-one match-based video games, which was not achievable by previous methods.

% Although we study the deck recommendation problem particularly in CCG, we expect our method to be easily applied to efficient starting item recommendations for other genres of games where: (1) the choice space of starting items is large; (2) the winning-effectiveness of starting items can be evaluated through some form of evaluation (e.g., simulations as in our study) in the training phase; (3) heuristic searches do not yield good results or require domain knowledge heavy to build, and intermediate solution evaluation needed by metaheuristic searches would take up computational resources more expensive than they could afford for large-scale or real-time application.






% In this paper, we propose a deck recommendation system named \textit{Q-DeckRec} whose goal is to efficiently identify winning-effective decks against specific opponents. We first model the deck building problem as solving a COP by sequential decision making, then learn a search policy by leveraging an RL algorithm on a set of "training" problem instances. The key idea is to generalize a search policy in order to find winning-effective decks and find them quickly for future problem instances. Thus, Q-DeckRec is suitable to deploy for large-scale or real-time application, e.g., an online CCG's backend to recommend winning-effective decks to a population of online players, a deck analysis website to serve hundreds of online visitors' deck building requests, or large-scale deck balancing tests. 

% The contributions of the paper are:
% \begin{enumerate}
%     \item we formulate the deck build problem as a combinatorial optimization problem (COP);
%     \item we propose Q-DeckRec, an algorithm which learns a search policy for solving deck building problem instances quickly;
%     \item we conduct experiments to demonstrate Q-DeckRec's suitability for large-scale or real-time application. The results show that after a training phase Q-DeckRec is able to build highly winning-effective decks within 9.63 seconds of CPU time, which is not achievable by other methods.
% \end{enumerate}



% \section{Limitations and Future Works}\label{sec:limitations}
% % Therefore, the optimal decks obtained in our experiments cannot be directly recommended to human players.
% In order to help human players, Q-DeckRec relies on AI proxies which can accurately model players' play styles. The current used AI proxy is only based on a greedy heuristic rather than trained on human play traces. Training human-like AI proxies and integrating them to Q-DeckRec will be an important direction in our future works.

% We can also improve sample efficiency in Q-DeckRec. Currently, a training episode starts with a random $s_0 = \{\vect{x}_p^{(0)}, \vect{x}_o, 0\}$. Were it generated from a card distribution learned from real matches, Q-DeckRec can focus on exploring in a smaller state space.

% Next, as online CCGs often release patches to introduce new cards and modify existing cards' in-game effects, we would like to investigate how Q-DeckRec can transfer and update its knowledge without totally re-training the model~\cite{taylor2009transfer}.

% Lastly, if the problem is extended to recommend winning-effective decks against a group of opponent decks $\{\vect{x}_{oi}\}^k_{i=1}$, there remains a question of how to design the feature representation of state-action pairs. Naive feature representations for the opponent deck group could be simply concatenating $\{\vect{x}_{oi}\}^k_{i=1}$. However this creates a large feature space which may not be efficient for learning. A more advanced feature representation may represent the opponent deck group in a continuous vector space, similar to word-embedding techniques from Natural Language Processing (NLP)~\cite{mikolov2013distributed}. We intend to investigate all of these in the future.


% two points not included% Another limitation is that players simply receive recommended winning-effective decks without further instructions on how to use them. It would be interesting to investigate how to generate informative instructions based on the input AI proxies, the opponent's deck and the recommended deck. As far as we know, scarce works have studied relevant problems~\cite{yang2013extracting,yang2013knowledge,de2016generating,harrison2017rationalization}. 

% Another limitation is that players simply receive recommended winning-effective decks without further instructions on how to use them. It would be interesting to investigate how to generate informative instructions based on the input AI proxies, the opponent's deck and the recommended deck. As far as we know, scarce works have studied relevant problems~\cite{yang2013extracting,yang2013knowledge,de2016generating,harrison2017rationalization}. 

% At last, the wall time that GA needs to identify on winning-effective decks is generally shorter in our experiments than that reported previous works which claimed to take a few days~\cite{garcia2016evolutionary,bjorke2017deckbuilding}. We suspect a few factors causing the difference. The main factor might be that our used server is much more powerful with more CPU cores, allowing faster match simulation. Other factors could include different simulators and AI bots, different numbers of total available cards and sizes of decks, and different libraries for GA implementation. In the future, we plan to conduct experiments on more kinds of settings.

% --------------------------------------------------------------------


% Ideas proposed for deck building mainly fall into two categories: heuristics and metaheuristic searches. First, some heuristic methods decide which cards to include based on the popularity of cards from  historical data~\cite{frankkarsten,willfancher}. The underlying intuition is that popularly favored cards are very likely to be strong ones. Stiegler et al. propose a utility system to search deck with more types of game-specific heuristics besides card popularity, including mana curve, strategic parameters, cost effectiveness and card synergies~\cite{stiegler2016hearthstone}. However, all heuristics methods require intensive human knowledge, lack flexibility to adapt to a wide variety of opponents decks. As far as we know, \textit{Genetic Algorithm} (GA)~\cite{holland1992adaptation} has been the only metaheuristic search algorithm for deck building~\cite{garcia2016evolutionary,bjorke2017deckbuilding}. In their works, the fitness value is the average win rate of a candidate deck against a group of opponent decks while AI bots are used as a proxy for human play. However, we note in their results that a single run of GA for a particular deck building problem instance took hours or days to reach a winning-effective deck~\cite{garcia2016evolutionary,bjorke2017deckbuilding}. This is because each fitness evaluation requires obtaining a win rate based on a number of simulated matches. The complicated in-game rules also make the simulation computationally expensive. Therefore, non-learning search algorithms like GA are not practical for any large-scale or real-time deck recommendation task.

% \subsection{Collectible Card Game Overview}
% Given the various characteristics of cards, the randomness of card drawn processes, and hidden information from opponents, playing a CCG well is a challenging task for human players. Due to the same reasons, CCGs has also attracted artificial intelligence research. For example, Ling et al. proposed a generative model to automatically generate card specifications~\cite{ling2016latent}. Cowling et al. investigated different Monte Carlo tree search methods to deal with the imperfect information in the Magic: the Gathering game ~\cite{cowling2012ensemble}. Santos et. al. and Taralla designed AI bots for playing Hearthstone~\cite{santos2017monte,taralla2015learning}. CCGs have also been used as a testament of a framework to automatically detect design issues of new games~\cite{osborn2013modular}.
% Although in-game rules may vary to some extent, we focus on those CCGs similar to Hearthstone because its pattern is common and the simulator we use for our experiments is also based on it. 

% Each match is one-vs-one and turn-based. Each player starts with an amount of health and the goal is to destroy the opponent's health first. Each player is asked to construct a \textit{deck} of a fixed number of cards before the actual match. During a player's turn, he plays the cards drawn from his own deck as per their rules and limited by his resource. Cards can be mainly categorized as \textit{spells} and \textit{minions}. Spells are played, creating an effect on the battlefield, and then are discarded. Minions, on the other hand, stay in play, and can be used to attack the enemy or other minions. There usually exist several deck archetypes in a CCG and no single deck can triumph over others universally (see the example of Aggro and Control decks in Section~\ref{sec:introduction}). 
% Deck archetypes usually keep evolving as the game community discovers new strategies and adapts to old ones. 

% For example, \textit{Aggro} decks rely on explosive damage in the early game in order to surge to victory before the opponent has time to counter them. On the other hand, \textit{Control} decks focus on controlling the early game in order to survive through to the later rounds, where they can use a string of powerful spells, or a steady flow of larger minions to overwhelm the opponent.

% Although a player cannot know what cards constitute the deck of his opponent before the match, he could make predictions of opponent decks and propose his deck in advance to reflect his winning philosophies. In other deck building applications, such as recommending decks in a practice mode and deck balancing tests, opponent decks can also be assumed to be known at the time of deck building.


% A deck recommendation system for the purpose of deck building can benefit players and game developers in several ways. First, it can ease choices made by players in deck building. Players may also learn new strategies of deck building and practice their skills based on recommended decks. Second, such a system can be useful to increase player's engagement, by controlling match outcomes to keep players interested ~\cite{chen2017eomm,chen2015analytics}. Deploying a deck recommendation system in certain modes (e.g., a practice mode) could help re-engage players who are frustrated with not being able to build winning-effective decks.  Last, a deck recommendation system is also useful for debugging games, from a game developer's perspective. For example,  balancing the power of cards is an important topic in CCGs~\cite{ham2010rarity} or similar games~\cite{mahlmann2012evolving}. The game developer can use a deck recommendation system to check whether certain combinations of cards are overly powerful or weak.